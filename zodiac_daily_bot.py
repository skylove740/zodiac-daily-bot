# zodiac_daily_bot.py

import os
import base64
import openai
from openai import OpenAI
import requests
from datetime import datetime, timedelta, timezone
from PIL import Image, ImageDraw, ImageFont
from apscheduler.schedulers.background import BackgroundScheduler
from flask import Flask
import re
import base64
from dotenv import load_dotenv
from moviepy.editor import ImageClip, concatenate_videoclips, AudioFileClip, CompositeVideoClip, VideoFileClip
from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from pytz import timezone
from zoneinfo import ZoneInfo
import feedparser
import json
from newsdataapi import NewsDataApiClient
from bs4 import BeautifulSoup
import textwrap
import glob
import random
import numpy as np
from typing import List, Dict, Any
import html


load_dotenv()

os.environ["IMAGEMAGICK_BINARY"] = "/usr/bin/convert"
YOUTUBE_SCOPES = ["https://www.googleapis.com/auth/youtube.upload"]

timestamps = {}

# ========== í™˜ê²½ ì„¤ì • ==========
# API Key ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
NEWSDATA_API_KEY = os.getenv("NEWSDATA_API_KEY")
api = NewsDataApiClient(apikey=NEWSDATA_API_KEY)


IG_ACCESS_TOKEN = os.environ.get("IG_ACCESS_TOKEN")
IG_USER_ID = os.environ.get("IG_USER_ID")

ZODIACS = ["ì¥", "ì†Œ", "í˜¸ë‘ì´", "í† ë¼", "ìš©", "ë±€", "ë§", "ì–‘", "ì›ìˆ­ì´", "ë‹­", "ê°œ", "ë¼ì§€"]
ZODIACS_star = ["ë¬¼ë³‘", "ë¬¼ê³ ê¸°", "ì–‘", "í™©ì†Œ", "ìŒë‘¥ì´", "ê²Œ", "ì‚¬ì", "ì²˜ë…€", "ì²œì¹­", "ì „ê°ˆ", "ì‚¬ìˆ˜", "ì—¼ì†Œ"]

# BASE_DIR = "zodiac-daily-bot"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
BG_DIR = os.path.join(BASE_DIR, "backgrounds")
OUT_DIR = os.path.join(BASE_DIR, "results")
# BG_DIR = "backgrounds"
# OUT_DIR = "results"
FONT_PATH = os.path.join(BASE_DIR, "fonts", "ë‚˜ëˆ”ì†ê¸€ì”¨ ëŠë¦¿ëŠë¦¿ì²´.ttf")
FONT_SIZE = 90
TEXT_BOX = (190, 700, 830, 1500)  # (x1, y1, x2, y2) ì¢Œí‘œ


####### ê¸°ì‚¬ ê´€ë ¨ ì„¤ì •

# ë‰´ìŠ¤ ê´€ë ¨ íŒŒì¼ ê²½ë¡œ ì„¤ì •
NEWS_SOURCE_FILE = os.path.join(BASE_DIR, "news_source_id_div.json")
ARTICLES_FILE = os.path.join(BASE_DIR, f"us_newsdata_articles.json")
UNKNOWN_SOURCE_FILE = os.path.join(BASE_DIR, "unknown_sources.txt")

# INTRO_BG = os.path.join(BG_DIR, "intro_bg.png")
# BODY_BG = os.path.join(BG_DIR, "body_bg.png")
OUTRO_BG = os.path.join(BG_DIR, "outro_bg.png")

OUTPUT_INTRO = os.path.join(OUT_DIR, "intro_output.jpg")
OUTPUT_BODY = os.path.join(OUT_DIR, "body_output")
OUTPUT_OUTRO = os.path.join(OUT_DIR, "outro_output.jpg")


os.makedirs(BG_DIR, exist_ok=True)
os.makedirs(OUT_DIR, exist_ok=True)



# ========== ê²½ì œ ë‰´ìŠ¤ ê´€ë ¨ í•¨ìˆ˜ë“¤ ==========

def get_news_from_html():
    # news_source_id_div.json ë¶ˆëŸ¬ì˜¤ê¸°
    with open(NEWS_SOURCE_FILE, "r", encoding="utf-8") as f:
        source_rules = json.load(f)

    # source_name â†’ rule ë§¤í•‘
    source_map = {item["source_name"].strip(): item for item in source_rules}

    # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
    with open(ARTICLES_FILE, "r", encoding="utf-8") as f:
        articles = json.load(f)

    collected_articles = []  # ì „ì²´ ê¸°ì‚¬ ë³¸ë¬¸
    unknown_sources = set()  # ëª» ì°¾ì€ source_name

    for article in articles:
        source_name = article.get("source_name").strip()
        link = article.get("link")
        source_url = article.get("source_url", "")

        if not link or not source_name:
            print("Missing link or source_name, skipping...")
            continue

        print(f"Processing: {source_name} | {link}")

        try:
            # HTML ê°€ì ¸ì˜¤ê¸°
            response = requests.get(link, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            if source_name in source_map:
                rule = source_map[source_name]
                attr = rule.get("attribute")
                value = rule.get("value", "")

                extracted_text = ""

                if attr == "article":
                    # <article> íƒœê·¸ ëª¨ë‘
                    articles_html = soup.find_all("article")
                    extracted_text = "\n".join(a.get_text(strip=True) for a in articles_html)

                elif attr == "class":
                    elems = soup.find_all(class_=value)
                    extracted_text = "\n".join(e.get_text(strip=True) for e in elems)

                elif attr == "id":
                    elems = soup.find_all(id=value)
                    extracted_text = "\n".join(e.get_text(strip=True) for e in elems)

                else:
                    print(f"Unknown attribute for {source_name}: {attr}")

                collected_articles.append({
                    "source_name": source_name,
                    "link": link,
                    "content": extracted_text.strip()
                })

            else:
                # source_nameì´ rulesì— ì—†ìŒ
                unknown_sources.add((source_name, source_url))

        except Exception as e:
            print(f"Error processing {link}: {e}")
            unknown_sources.add((source_name, source_url))

    # unknown_sources.txt íŒŒì¼ ì €ì¥ (ì¶”ê°€ ëª¨ë“œ)
    if unknown_sources:
        lines = [f"{name} | {url}" for name, url in sorted(unknown_sources)]
        if not os.path.exists(UNKNOWN_SOURCE_FILE):
            with open(UNKNOWN_SOURCE_FILE, "w", encoding="utf-8") as f:
                f.write("\n".join(lines) + "\n")
        else:
            with open(UNKNOWN_SOURCE_FILE, "a", encoding="utf-8") as f:
                f.write("\n".join(lines) + "\n")

    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ì „ì²´ ê¸°ì‚¬ ë³¸ë¬¸ ===")
    for idx, art in enumerate(collected_articles, start=1):
        print(f"[{idx}] {art['source_name']} ({art['link']})")
        print(art["content"][:400], "...\n")  # ì•ë¶€ë¶„ 500ìë§Œ ì¶œë ¥

    print("\n=== ëª» ì°¾ì€ source_name ëª©ë¡ ===")
    for su, s in sorted(unknown_sources):
        print("-", su,":",s)

    return collected_articles

# ì‹œê°„ ë²”ìœ„ ì„¤ì • (ì „ë‚  7ì‹œ 30ë¶„ ~ í˜„ì¬ ì‹œê°„)
def get_time_range_iso():
    now = datetime.utcnow() + timedelta(hours=9)
    start = now - timedelta(days=1)
    start = start.replace(hour=7, minute=30, second=0, microsecond=0)
    return start.isoformat(), now.isoformat()

def fetch_newsdata_articles(q=None, country=None, language=None, category=None):
    # ìµœì‹  ë‰´ìŠ¤ endpoint (/1/news)
    params = {}
    if country:
        params["country"] = country
    if language:
        params["language"] = language
    if category:
        params["category"] = category
    if q:
        params["q"] = q  # 'í…ŒìŠ¬ë¼' ê´€ë ¨ ê¸°ì‚¬ë§Œ í•„í„°ë§
    resp = api.news_api(**params)
    return resp.get("results", [])



RSS_FEEDS = {
    "kr": [
        "https://www.hankyung.com/feed/economy",
        "https://rss.etnews.com/Section901.xml",
        "https://www.mk.co.kr/rss/30000001/",
        "https://rss.edaily.co.kr/rss/economy.xml",
    ],
    "global": [
        "https://feeds.bbci.co.uk/news/business/rss.xml",
        "https://www.cnbc.com/id/10001147/device/rss/rss.html",
    ],
    "us": [
        "https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=20910258",  # CNBC ê²½ì œ
        "https://feeds.content.dowjones.io/wsj/business",  # WSJ ë¹„ì¦ˆë‹ˆìŠ¤ (ì˜ˆì‹œ URL)
    ]
}

def get_time_range():
    now = datetime.utcnow() + timedelta(hours=9)  # í•œêµ­ ì‹œê°„
    end_time = now
    start_time = now - timedelta(days=1)
    start_time = start_time.replace(hour=7, minute=30, second=0, microsecond=0)
    return start_time, end_time

def fetch_rss_articles(region):
    start_time, end_time = get_time_range()
    articles = []
    for feed_url in RSS_FEEDS.get(region, []):
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            published = entry.get("published_parsed")
            if not published:
                continue
            pub_time = datetime(*published[:6])
            if start_time <= pub_time <= end_time:
                articles.append({
                    "title": entry.title,
                    "link": entry.link,
                    "published": pub_time.isoformat(),
                    "summary": entry.get("summary", ""),
                })
    return articles

def save_articles(region, source, articles):
    # date_str = datetime.now().strftime("%Y-%m-%d")
    filename = f"{region}_{source}_articles.json"
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"[SAVED] {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ â†’ {filename}")


def summarize_articles(articles, target):
    summarized_results = []

    for idx, art in enumerate(articles, start=1):
        if len(summarized_results) >= 3:
            break  # 3ê°œê¹Œì§€ë§Œ ìš”ì•½í•˜ê³  ë°˜ë³µ ì¤‘ì§€
        article = clean_emoji_text(art["content"])
        try:
            # GPTì—ê²Œ ìš”ì²­í•  í”„ë¡¬í”„íŠ¸
            prompt = (
                "ì•„ë˜ ê¸°ì‚¬ë¥¼ ì£¼ê°€ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”\n"
                "ëª¨ë“  ë‚´ìš©ì€ ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì—ì„œ ì¸ìš©í•´ì•¼ í•˜ê³ , ì—†ëŠ” ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ë©´ ì•ˆë©ë‹ˆë‹¤.\n"
                "ê° ì¤„ì€ ê°„ê²°í•˜ê³  ëª…í™•í•´ì•¼ í•˜ë©°, ì£¼ì œë¥¼ ë¶„ëª…íˆ ë“œëŸ¬ë‚´ì•¼ í•©ë‹ˆë‹¤.\n"
                "ì›ë¬¸ ê·¸ëŒ€ë¡œì˜ ì–¸ì–´ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”\n\n"
                f"ê°„í˜¹ ê´‘ê³  ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. {target}ê³¼ ê´€ë ¨ ì—†ëŠ” ê´‘ê³  ë‚´ìš©ì˜ ê²½ìš°ì—” ì œì™¸í•˜ê³  ìš”ì•½í•´ ì£¼ì„¸ìš”.\n\n"
                f"ê¸°ì‚¬ ë‚´ìš©:\n{article}"
            )
            print("ê¸°ì‚¬ ê¸¸ì´ : ", len(article))

            if len(article) > 10:
                response = openai.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0
                    # max_tokens=300
                )

                summary = response.choices[0].message.content.strip()
                if len(summary) < 3:
                    print(f"[{idx}] ìš”ì•½ ì‹¤íŒ¨: ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
                    continue
                elif len(summary) > 50:
                    print(f"[{idx}] ìš”ì•½ : ìš”ì•½ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ìš”ì•½í•˜ê² ìŠµë‹ˆë‹¤.")
                    response = openai.chat.completions.create(
                        model="gpt-4.1",
                        messages=[
                            {"role": "system", "content": "ë‹¹ì‹ ì€ ê²½ì œ ë‰´ìŠ¤ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                            {"role": "user", "content": "í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ, ì—†ëŠ” ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ê³  ìš”ì•½í•´ ì£¼ì„¸ìš”. ê°€ëŠ¥í•œ ì£¼ê°€ì™€ ê´€ë ¨ ìˆì„ ë§Œí•œ ê²½ì œì ì¸ ë‚´ìš©ì€ ìš”ì•½ ì‹œì— í¬í•¨ì‹œì¼œ ì£¼ì„¸ìš”. ìµœì¢… ì¶œë ¥ì€ í•œê¸€ë¡œ ë²ˆì—­í•´ì„œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ìŒì˜ ìš”ì•½ëœ ê¸°ì‚¬ë¥¼ í•œê¸€ ê¸°ì¤€ 250ì ë‚´ë¡œ ë‹¤ì‹œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\nìš”ì•½ ê¸°ì‚¬ : " + summary}
                        ],
                        temperature=0
                        # max_tokens=300
                    )
                    summary = response.choices[0].message.content.strip()


                print(f"[{idx}] 1ì°¨ ìš”ì•½ : {summary}")


                response = openai.chat.completions.create(
                    model="gpt-4.1",
                    messages=[
                        {"role": "system", "content": "ëŒ€ë‹µì€ OK ë˜ëŠ” NOë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”."},
                        {"role": "user", "content": f"ì´ ìš”ì•½ì´ {target}ê³¼ ì§/ê°„ì ‘ì ìœ¼ë¡œ ê´€ë ¨ì´ ìˆëŠ” ê¸°ì‚¬ê°€ ì •ë§ ë§ë‚˜ìš”? {target}ì— ëŒ€í•œ ìµœì‹  ê¸°ì¤€ì˜ ì›¹ ì„œì¹˜ í™•ì¸ í›„ ë‹µë³€í•´ ì£¼ì„¸ìš”. ìš”ì•½ : {summary}"}
                    ],
                    temperature=0
                    # max_tokens=300
                )

                if response.choices[0].message.content.strip().lower() != "ok":
                    print(f"[{idx}] ìš”ì•½ ìƒëµ: {target}ê³¼ ê´€ë ¨ ì—†ëŠ” ê¸°ì‚¬ì…ë‹ˆë‹¤.")
                    continue

                response = openai.chat.completions.create(
                    model="gpt-4.1",
                    messages=[
                        {"role": "system", "content": "ëŒ€ë‹µì€ OK ë˜ëŠ” NOë¡œë§Œ ëŒ€ë‹µí•˜ì„¸ìš”."},
                        {"role": "user", "content": f"ì§€ê¸ˆ ì£¼ì–´ì§€ëŠ” 'ìš”ì•½'ê³¼ ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì´ ì£¼ì–´ì§€ëŠ” 'ë°°ì—´' ì•ˆì— ìˆë‚˜ìš”?\n'ìš”ì•½' : {summary}\n'ë°°ì—´' : {summarized_results}\n"}
                    ],
                    temperature=0
                    # max_tokens=300
                )

                if response.choices[0].message.content.strip().lower() == "ok":
                    print(f"[{idx}] ìš”ì•½ ìƒëµ: ì¤‘ë³µë˜ëŠ” ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤.")
                    print(f"[{idx}] ìš”ì•½ : {summary}")
                    print("ë°°ì—´ : ", summarized_results)
                    continue

                summarized_results.append(summary)

                print(f"[{idx}] ìš”ì•½ ì™„ë£Œ:")
                print(summary)
                print("=" * 50)
                print("ìš”ì•½ ê¸¸ì´ : ", len(summary))
            else:
                print(f"[{idx}] ìš”ì•½ ìƒëµ: ê¸°ì‚¬ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
                # summarized_results.append("ê¸°ì‚¬ ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"[{idx}] ìš”ì•½ ì‹¤íŒ¨: {e}")
            # summarized_results.append("ìš”ì•½ ì‹¤íŒ¨")

    return summarized_results

# ===== ìœ í‹¸: í…Œë‘ë¦¬ + ë°˜íˆ¬ëª… ë°•ìŠ¤ í…ìŠ¤íŠ¸ =====
def draw_text_with_box(img, text, position, font, text_color, box_color, outline_color):
    # drawëŠ” ì›ë³¸ ì´ë¯¸ì§€ì˜ draw ê°ì²´
    draw = ImageDraw.Draw(img, "RGBA")
    text_bbox = draw.textbbox(position, text, font=font)
    box_padding = 10
    box_coords = (
        text_bbox[0] - box_padding,
        text_bbox[1] - box_padding,
        text_bbox[2] + box_padding,
        text_bbox[3] + box_padding
    )
    # 1. ì˜¤ë²„ë ˆì´ ì´ë¯¸ì§€ ìƒì„±
    overlay = Image.new("RGBA", img.size, (255, 255, 255, 0))
    overlay_draw = ImageDraw.Draw(overlay)
    overlay_draw.rectangle(box_coords, fill=box_color)
    # 2. ì›ë³¸ê³¼ ì˜¤ë²„ë ˆì´ í•©ì„±
    img = Image.alpha_composite(img, overlay)
    draw = ImageDraw.Draw(img, "RGBA")
    # 3. í…Œë‘ë¦¬ íš¨ê³¼
    x, y = position
    for dx in [-1, 1]:
        for dy in [-1, 1]:
            draw.text((x + dx, y + dy), text, font=font, fill=outline_color)
    # 4. ë³¸ë¬¸ í…ìŠ¤íŠ¸
    draw.text(position, text, font=font, fill=text_color)
    return img  # í•„ìš”ì‹œ ë°˜í™˜

def delete_body_images():
    """
    OUTPUT_BODYë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  jpg íŒŒì¼ ì‚­ì œ
    """
    pattern = f"{OUTPUT_BODY}*.jpg"
    files = glob.glob(pattern)
    for file in files:
        try:
            os.remove(file)
            print(f"ì‚­ì œë¨: {file}")
        except Exception as e:
            print(f"íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {file} - {e}")

# ===== ì¸íŠ¸ë¡œ ì´ë¯¸ì§€ ìƒì„± =====
def create_intro_image_news(target_en, target_kr):
    # ë³¸ë¬¸ ì´ë¯¸ì§€ ìƒì„± ì „ ê¸°ì¡´ ì´ë¯¸ì§€ ì‚­ì œ
    delete_body_images()

    date_str = datetime.now().strftime("%Y.%m.%d")
    lines = [date_str, target_kr, "ê´€ë ¨ ë‰´ìŠ¤"]

    intro_bg = os.path.join(BG_DIR, "intro_bg_"+target_en.split(" ")[0]+".png")
    # intro_bgê°€ ì—†ìœ¼ë©´ fallbackìœ¼ë¡œ ëŒ€ì²´
    if not os.path.exists(intro_bg):
        print(f"[ê²½ê³ ] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {intro_bg}, ëŒ€ì²´ ì´ë¯¸ì§€ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        intro_bg = os.path.join(BG_DIR, "intro_bg_business.png")

    img = Image.open(intro_bg).convert("RGBA")
    W, H = img.size
    draw = ImageDraw.Draw(img, "RGBA")

    # í°íŠ¸ í¬ê¸° ì¡°ì • (ì „ì²´ ë†’ì´ì˜ ì ˆë°˜ ì°¨ì§€)
    font_size = 10
    font = ImageFont.truetype(FONT_PATH, font_size)
    while True:
        font = ImageFont.truetype(FONT_PATH, font_size)
        total_height = sum([draw.textbbox((0,0), line, font=font)[3] for line in lines])
        if total_height >= H * 0.5:
            break
        font_size += 2

    y_offset = (H - total_height) // 2
    for line in lines:
        w, h = draw.textsize(line, font=font)
        x = (W - w) // 2
        img = draw_text_with_box(img, line, (x, y_offset), font, "white", (0, 0, 0, 150), "black")
        draw = ImageDraw.Draw(img, "RGBA")  # draw ê°ì²´ ê°±ì‹ 
        y_offset += h + 10

    img.convert("RGB").save(OUTPUT_INTRO)

def split_korean_sentences(text):
    # í•œê¸€ ê¸°ì¤€ ë¬¸ì¥ ë¶„ë¦¬ (ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ë’¤ì— ì¤„ë°”ê¿ˆ)
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())
    # ë¹ˆ ë¬¸ì¥ ì œê±°
    return [s for s in sentences if s]


# ===== ë³¸ë¬¸ ì´ë¯¸ì§€ ìƒì„± =====
def create_body_image(text, idx, target):
    # 1. idx ë¶™ì´ê¸°
    text = f"{(idx+1)}) {text}"
    # 2. ë¬¸ì¥ ë¶„ë¦¬
    sentences = split_korean_sentences(text)
    # ğŸ”¥ HTML escape ì œê±°
    sentences = [html.unescape(s).strip() for s in sentences]

    # í•œ ë¬¸ì¥ì”© ë¬¶ê¸°
    pages = []
    buffer = ""
    # for i in range(len(sentences)):
    #     page_text = sentences[i]
    #     pages.append(page_text)
    for sent in sentences:
        # ğŸ”¥ ìˆ«ì + '.' í˜•ì‹ì¸ì§€ ì²´í¬ (ì˜ˆ: "1.", "2.", "10.")
        if re.fullmatch(r"\d+\.", sent):
            # ê¸°ì¡´ bufferê°€ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ë¨¼ì € ì €ì¥
            if buffer.strip():
                pages.append(buffer.strip())
                buffer = ""
            # ë„˜ë²„ë§ì€ ë‹¨ë… í˜ì´ì§€
            pages.append(sent)
            continue

        # ------- ì¼ë°˜ ë¬¸ì¥ ì²˜ë¦¬ -------
        # í˜„ì¬ ë²„í¼ + ë¬¸ì¥ì„ í•©ì³¤ì„ ë•Œ 50ì ì´í•˜ì´ë©´ ê°™ì€ í˜ì´ì§€ë¡œ ë¬¶ê¸°
        if len(buffer) + len(sent) <= 50:
            if buffer == "":
                buffer = sent
            else:
                buffer += " " + sent
        else:
            # 50ìë¥¼ ë„˜ìœ¼ë©´ í˜„ì¬ í˜ì´ì§€ ì €ì¥í•˜ê³  ìƒˆ í˜ì´ì§€ ì‹œì‘
            if buffer.strip():
                pages.append(buffer.strip())
            buffer = sent

    # ë²„í¼ ì•ˆ ë‚¨ì•„ìˆìœ¼ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ ì €ì¥
    if buffer.strip():
        pages.append(buffer.strip())


    saved_files = []
    for page_num, page_text in enumerate(pages, start=1):
        body_bg = os.path.join(BG_DIR, "body_bg_"+target+".png")
        # intro_bgê°€ ì—†ìœ¼ë©´ fallbackìœ¼ë¡œ ëŒ€ì²´
        if not os.path.exists(body_bg):
            print(f"[ê²½ê³ ] íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {body_bg}, ëŒ€ì²´ ì´ë¯¸ì§€ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            body_bg = os.path.join(BG_DIR, "body_bg_tesla.png")
        
        img = Image.open(body_bg).convert("RGBA")
        W, H = img.size
        draw = ImageDraw.Draw(img, "RGBA")

        # í°íŠ¸ í¬ê¸° ë§ì¶”ê¸°
        font_size = 50
        while True:
            font = ImageFont.truetype(FONT_PATH, font_size)
            wrapped = textwrap.fill(page_text, width=20)
            tw, th = draw.multiline_textsize(wrapped, font=font, spacing=10)
            if tw > W * 0.9 or th > H * 0.9:
                font_size -= 2
                font = ImageFont.truetype(FONT_PATH, font_size)
                wrapped = textwrap.fill(page_text, width=20)
                break
            font_size += 2

        tw, th = draw.multiline_textsize(wrapped, font=font, spacing=10)
        x = (W - tw) // 2
        y = (H - th) // 2

        # ë°˜íˆ¬ëª… ë°•ìŠ¤
        box_coords = (x - 20, y - 20, x + tw + 20, y + th + 20)
        overlay = Image.new("RGBA", img.size, (255, 255, 255, 0))
        overlay_draw = ImageDraw.Draw(overlay)
        overlay_draw.rectangle(box_coords, fill=(0, 0, 0, 150))
        img = Image.alpha_composite(img, overlay)
        draw = ImageDraw.Draw(img, "RGBA")

        # í…Œë‘ë¦¬ + í…ìŠ¤íŠ¸
        for dx in [-1, 1]:
            for dy in [-1, 1]:
                draw.multiline_text((x + dx, y + dy), wrapped, font=font, fill="black", spacing=10)
        draw.multiline_text((x, y), wrapped, font=font, fill="white", spacing=10)

        out_path = f"{OUTPUT_BODY}{str(idx)}_{page_num}.jpg"
        img.convert("RGB").save(out_path)
        saved_files.append(out_path)
    return saved_files

# ===== ì•„ì›ƒíŠ¸ë¡œ ì´ë¯¸ì§€ =====
def create_outro_image():
    # outro_bg = os.path.join(BG_DIR, "outro_bg_"+target+".png")
    img = Image.open(OUTRO_BG)
    img.save(OUTPUT_OUTRO)


def extract_numbers(filename):
    # body_output{idx}_{page}.jpgì—ì„œ idxì™€ pageë¥¼ ì¶”ì¶œ
    m = re.search(r'body_output(\d+)_(\d+)\.jpg', filename)
    if m:
        return int(m.group(1)), int(m.group(2))
    return 0, 0

def create_youtube_shorts_video(intro_path, body_dir, outro_path, bgm_path, output_path):
    # ë³¸ë¬¸ ì´ë¯¸ì§€ë“¤: body_output{idx}_{page}.jpg í˜•ì‹ ëª¨ë‘ ì‚¬ìš©
    body_images = sorted(
        [f for f in os.listdir(body_dir) if f.startswith("body_output") and f.endswith(".jpg")],
        key=extract_numbers
    )
    num_intro = 1
    num_body = len(body_images)
    num_outro = 1
    total_images = num_intro + num_body + num_outro

    # ê¸°ë³¸ê°’
    intro_duration = 3
    body_duration = 3
    outro_duration = 2

    # ì´ ê¸¸ì´ ê³„ì‚° ë° ì¡°ì •
    total_duration = intro_duration + body_duration * num_body + outro_duration
    target_duration = 60

    # ë³¸ë¬¸ì´ ë§ì„ ë•Œ ìë™ ì¡°ì •
    if total_duration > target_duration:
        # ë³¸ë¬¸ ê¸¸ì´ ìµœì†Œ 2ì´ˆë¡œ ì¡°ì •
        body_duration = max(2, (target_duration - 6) // num_body)
        # ì¸íŠ¸ë¡œ/ì•„ì›ƒíŠ¸ë¡œëŠ” 2~4ì´ˆ ì‚¬ì´ë¡œ ì¡°ì •
        intro_duration = min(max(2, intro_duration), 4)
        outro_duration = min(max(2, outro_duration), 4)
        # ë‹¤ì‹œ ì´ ê¸¸ì´ ê³„ì‚°
        total_duration = intro_duration + body_duration * num_body + outro_duration
        # ë‚¨ì€ ì‹œê°„ ë¶„ë°°
        if total_duration < target_duration:
            remain = target_duration - (body_duration * num_body)
            # ì¸íŠ¸ë¡œ/ì•„ì›ƒíŠ¸ë¡œì— ë‚¨ì€ ì‹œê°„ ë¶„ë°° (ìµœëŒ€ 4ì´ˆê¹Œì§€)
            intro_duration = min(4, remain // 2)
            outro_duration = min(4, remain - intro_duration)
        # ìµœì¢… ì²´í¬
        total_duration = intro_duration + body_duration * num_body + outro_duration
        if total_duration > target_duration:
            # ì•„ì›ƒíŠ¸ë¡œë¶€í„° ì¤„ì„
            diff = total_duration - target_duration
            outro_duration = max(2, outro_duration - diff)

    clips = []

    # 1. ì¸íŠ¸ë¡œ ì´ë¯¸ì§€
    intro_clip = ImageClip(intro_path).set_duration(intro_duration)
    clips.append(intro_clip)

    # 2. ë³¸ë¬¸ ì´ë¯¸ì§€ë“¤
    for img_file in body_images:
        img_path = os.path.join(body_dir, img_file)
        body_clip = ImageClip(img_path).set_duration(body_duration)
        clips.append(body_clip)

    # 3. ì•„ì›ƒíŠ¸ë¡œ ì´ë¯¸ì§€
    outro_clip = ImageClip(outro_path).set_duration(outro_duration)
    clips.append(outro_clip)

    # 4. ì„¸ë¡œ(9:16) ìœ íŠœë¸Œ ì‡¼ì¸  ì‚¬ì´ì¦ˆ ë§ì¶”ê¸°
    clips = [clip.resize(height=1920).resize(width=1080) for clip in clips]

    # 5. ì˜ìƒ í•©ì¹˜ê¸°
    final_clip = concatenate_videoclips(clips, method="compose")

    # 6. BGM ì„¤ì •
    bgm = AudioFileClip(bgm_path).volumex(0.5)  # ë°°ê²½ìŒì•… ë³¼ë¥¨ ì¡°ì ˆ
    final_clip = final_clip.set_audio(bgm.set_duration(final_clip.duration))

    # 7. ì €ì¥
    final_clip.write_videofile(output_path, fps=30, codec='libx264', audio_codec='aac')

# ===== í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ =====
def create_caption_image(text, output_path, size=(1080, 1920), font_path=None, font_size=50):
    """
    ë°˜íˆ¬ëª… ë°•ìŠ¤ + ì¤‘ì•™ í…ìŠ¤íŠ¸ PNG ì´ë¯¸ì§€ ìƒì„±
    """
    img = Image.new("RGBA", size, (0, 0, 0, 0))  # ì™„ì „ íˆ¬ëª… ë°°ê²½
    draw = ImageDraw.Draw(img)

    # ì´ë¯¸ì§€ í¬ê¸°
    image_width, image_height = img.size

    max_text_height = image_height * 0.4
    max_text_width = image_width * 0.8

    # í°íŠ¸ ë¡œë”©
    font_size_init = 10
    if font_path:
        font_size = font_size_init
        while True:
            font = ImageFont.truetype(font_path, font_size)
            # í…ŒìŠ¤íŠ¸ ì¤„ë°”ê¿ˆ
            lines = wrap_text_by_pixel(text, font, max_text_width, draw)
            line_heights = [draw.textbbox((0,0), line, font=font)[3] for line in lines]
            total_height = sum(line_heights) + 10*(len(lines)-1)
            if total_height >= max_text_height or font_size > 200:
                break
            font_size += 2
    else:
        font = ImageFont.load_default()
        lines = wrap_text_by_pixel(text, font, max_text_width, draw)
        line_heights = [draw.textbbox((0,0), line, font=font)[3] for line in lines]

    spacing = 10
    total_height = sum(line_heights) + spacing*(len(lines)-1)

    # ë°•ìŠ¤ ì˜ì—­
    box_width = max_text_width + 40
    box_height = total_height + 40
    box_x = (size[0] - box_width)//2
    box_y = (size[1] - box_height)//2

    # ë°˜íˆ¬ëª… ë°•ìŠ¤
    draw.rectangle(
        (box_x, box_y, box_x + box_width, box_y + box_height),
        fill=(0, 0, 0, 150)
    )

    # í…ìŠ¤íŠ¸ ì¤‘ì•™ ì •ë ¬
    y_text = box_y + 20
    for line, h in zip(lines, line_heights):
        w = draw.textbbox((0, 0), line, font=font)[2]
        x = (size[0] - w)//2
        # í…Œë‘ë¦¬ íš¨ê³¼
        for dx in [-1, 1]:
            for dy in [-1, 1]:
                draw.text((x+dx, y_text+dy), line, font=font, fill=(0,0,0,255))
        draw.text((x, y_text), line, font=font, fill=(255,255,255,255))
        y_text += h + spacing

    # PNGë¡œ ì €ì¥ (ë°˜íˆ¬ëª… ìœ ì§€)
    img.save(output_path, format="PNG")


# ===== í”½ì…€ ê¸°ë°˜ ì¤„ë°”ê¿ˆ =====
def wrap_text_by_pixel(text, font, max_width, draw):
    words = text.split()
    lines = []
    current_line = ""
    for word in words:
        test_line = f"{current_line} {word}".strip()
        if draw.textlength(test_line, font=font) <= max_width:
            current_line = test_line
        else:
            if current_line:
                lines.append(current_line)
            current_line = word
    if current_line:
        lines.append(current_line)
    return lines

# ===== ë©”ëª¨ë¦¬ ê¸°ë°˜ ìº¡ì…˜ ì´ë¯¸ì§€ ìƒì„± =====
def create_caption_image_array(text, size=(1080, 1920), font_path=None):
    text = html.unescape(text or "")
    
    img = Image.new("RGBA", size, (0,0,0,0))
    draw = ImageDraw.Draw(img)

    image_width, image_height = img.size
    max_text_height = image_height * 0.4
    max_text_width = image_width * 0.8

    # í°íŠ¸ ë¡œë”©
    font_size = 10
    if font_path:
        while True:
            font = ImageFont.truetype(font_path, font_size)
            lines = wrap_text_by_pixel(text, font, max_text_width, draw)
            line_heights = [draw.textbbox((0,0), line, font=font)[3] for line in lines]
            # line_heights = [(font.getbbox(line)[3] - font.getbbox(line)[1]) for line in lines]
            total_height = sum(line_heights) + 10*(len(lines)-1)
            # max_width = max([draw.textlength(line, font=font) for line in lines])
            max_width = max([font.getlength(line) for line in lines])
            if total_height >= max_text_height or max_width >= max_text_width or font_size > 200:
                break
            font_size += 2
    else:
        font = ImageFont.load_default()
        lines = wrap_text_by_pixel(text, font, max_text_width, draw)
        line_heights = [draw.textbbox((0,0), line, font=font)[3] for line in lines]

    spacing = 10
    total_height = sum(line_heights) + spacing*(len(lines)-1)

    # ë°•ìŠ¤ ì˜ì—­
    box_width = max_text_width + 40
    box_height = total_height + 40
    box_x = (size[0] - box_width)//2
    box_y = (size[1] - box_height)//2

    # ë°˜íˆ¬ëª… ë°•ìŠ¤
    draw.rectangle(
        (box_x, box_y, box_x + box_width, box_y + box_height),
        fill=(0,0,0,150)
    )

    # í…ìŠ¤íŠ¸ ì¤‘ì•™ ì •ë ¬
    y_text = box_y + 20
    for line, h in zip(lines, line_heights):
        # w = draw.textbbox((0,0), line, font=font)[2]
        bbox = font.getbbox(line)
        w = bbox[2] - bbox[0]
        x = (size[0]-w)//2
        # í…Œë‘ë¦¬
        for dx in [-1,1]:
            for dy in [-1,1]:
                draw.text((x+dx, y_text+dy), line, font=font, fill=(0,0,0,255))
        draw.text((x, y_text), line, font=font, fill=(255,255,255,255))
        y_text += h + spacing

    # PIL -> Numpy Array (MoviePy ImageClip ì‚¬ìš© ê°€ëŠ¥)
    return np.array(img)

# ===== ë³¸ ì˜ìƒ ìƒì„± í•¨ìˆ˜ (ê°œì„ íŒ, summaryë³„ ë‹¤ë¥¸ ë°°ê²½ ì˜ìƒ + ë²ˆí˜¸ ë¶™ì´ê¸°) =====
def create_news_shorts_video_with_bgvideo_fast(
    target_en, summaries, bg_dir, out_dir, bgm_path, output_path,
    duration_per_caption=3, target_kr="í…ŒìŠ¬ë¼", font_path=None
):
    # 1. ë°°ê²½ ì˜ìƒ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    video_candidates = [f for f in os.listdir(bg_dir) if f.endswith(".mp4")]
    target_videos = [f for f in video_candidates if target_en.lower() in f.lower()]
    business_videos = [f for f in video_candidates if f.startswith("business")]

    if not target_videos and not business_videos:
        raise FileNotFoundError("ì ì ˆí•œ ë°°ê²½ ì˜ìƒ(mp4)ì´ backgrounds í´ë”ì— ì—†ìŠµë‹ˆë‹¤.")

    # summary ìˆ˜ ë§Œí¼ ë°°ê²½ ì˜ìƒ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    bg_video_list = []
    for i in range(len(summaries)+3):
        if i < len(target_videos):
            bg_video_list.append(target_videos[i])
        else:
            bg_video_list.append(random.choice(business_videos))

    # 2. intro/outro ì´ë¯¸ì§€
    create_intro_image_news(target_en, target_kr)
    intro_img_path = OUTPUT_INTRO
    if not os.path.exists(intro_img_path):
        intro_img_path = os.path.join(bg_dir, "intro_bg_tesla.png")
    outro_img_path = os.path.join(bg_dir, "outro_bg.png")

    clips = []

    # 3. ì¸íŠ¸ë¡œ
    intro_clip = ImageClip(intro_img_path).set_duration(3).resize((1080, 1920))
    clips.append(intro_clip)

    # 4. ë°°ê²½ ì˜ìƒ ê°ì²´ ë¯¸ë¦¬ ë¡œë”©
    bg_video_clips = [VideoFileClip(os.path.join(bg_dir, f)).resize((1080, 1920)) for f in bg_video_list]
    random.shuffle(bg_video_clips)
    bg_video_index = 0
    bg_video_start = 0

    # 5. ì „ì²´ ìë§‰ ìˆ˜ì— ë”°ë¼ ìë§‰ ì‹œê°„ ê³„ì‚°
    intro_duration = 2
    outro_duration = 2
    total_max_duration = 60
    available_caption_duration = total_max_duration - intro_duration - outro_duration

    total_sentences = 0
    for idx, summary in enumerate(summaries):
        numbered_summary = f"{idx+1}. {summary}"
        total_sentences += len(split_korean_sentences(numbered_summary))

    per_caption = available_caption_duration / total_sentences
    per_caption = max(2, min(4, per_caption))  # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šê²Œ

    # 6. ë³¸ë¬¸ ìƒì„±
    for idx, summary in enumerate(summaries):
        numbered_summary = f"{idx+1}. {summary}"
        sentences = split_korean_sentences(numbered_summary)

        for sent in sentences:
            caption_array = create_caption_image_array(sent, size=(1080, 1920), font_path=font_path)

            # â±ï¸ Duration ì„¤ì •
            if f"{idx+1}." == sent.strip():
                duration = 1
            else:
                duration = per_caption

            caption_clip = ImageClip(caption_array, transparent=True).set_duration(duration)

            remaining_time = duration
            bg_subclips = []

            # ğŸ” í•„ìš”í•œ ë§Œí¼ ë°°ê²½ ì˜ìƒ ì´ì–´ë¶™ì´ê¸°
            while remaining_time > 0:
                current_clip = bg_video_clips[bg_video_index]
                current_duration = current_clip.duration

                available = current_duration - bg_video_start
                use_duration = min(available, remaining_time)

                if use_duration <= 0:
                    bg_video_index = (bg_video_index + 1) % len(bg_video_clips)
                    bg_video_start = 0
                    continue

                subclip = current_clip.subclip(bg_video_start, bg_video_start + use_duration)
                bg_subclips.append(subclip)

                bg_video_start += use_duration
                remaining_time -= use_duration

                if bg_video_start >= current_duration:
                    bg_video_index = (bg_video_index + 1) % len(bg_video_clips)
                    bg_video_start = 0

            bg_clip = concatenate_videoclips(bg_subclips)
            comp_clip = CompositeVideoClip([bg_clip, caption_clip])
            clips.append(comp_clip)

    # 7. ì•„ì›ƒíŠ¸ë¡œ
    outro_clip = ImageClip(outro_img_path).set_duration(2).resize((1080, 1920))
    clips.append(outro_clip)

    # 8. ì „ì²´ ì˜ìƒ í•©ì„±
    final_clip = concatenate_videoclips(clips, method="compose")

    # 9. ë°°ê²½ìŒì•…
    if bgm_path and os.path.exists(bgm_path):
        bgm = AudioFileClip(bgm_path).volumex(0.5)
        final_clip = final_clip.set_audio(bgm.set_duration(final_clip.duration))

    # 10. ì €ì¥
    final_clip.write_videofile(output_path, fps=30, codec='libx264', audio_codec='aac')



# ============================ ìœ íŠ­ ì—…ë¡œë“œ ===========================
def upload_video_to_youtube_news(video_path, target_kr):
    global timestamps
    creds = Credentials.from_authorized_user_file("token.json", YOUTUBE_SCOPES)
    youtube = build("youtube", "v3", credentials=creds)

    now = datetime.now(ZoneInfo("Asia/Seoul"))
    date_str = now.strftime("%Yë…„ %mì›” %dì¼")

    body = {
        "snippet": {
            "title": f"{date_str} "+target_kr+" ê´€ë ¨ ë‰´ìŠ¤",  # ì˜ìƒ ì œëª©
            "description":
            f"{date_str} ì˜¤ëŠ˜ì˜ "+target_kr+" ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½ì…ë‹ˆë‹¤.\n\n#ë‰´ìŠ¤ìš”ì•½ #"+target_kr+" #"+target_kr+"ë‰´ìŠ¤ #ì˜¤ëŠ˜ì˜"+target_kr+" #ë‰´ìŠ¤ #shorts",
            "tags": ["ë‰´ìŠ¤", "ë‰´ìŠ¤ìš”ì•½", target_kr, target_kr+"ë‰´ìŠ¤", "ì˜¤ëŠ˜ì˜"+target_kr, "shorts"],
            "categoryId": "25"  # News & Politics
        },
        "status": {
            "privacyStatus": "public"  # ë˜ëŠ” unlisted, private
        }
    }

    media = MediaFileUpload(video_path,
                            chunksize=-1,
                            resumable=True,
                            mimetype="video/*")

    print("ğŸ“¤ ìœ íŠœë¸Œ ì—…ë¡œë“œ ì‹œì‘...")
    request = youtube.videos().insert(part="snippet,status",
                                      body=body,
                                      media_body=media)
    response = None

    while response is None:
        status, response = request.next_chunk()
        if status:
            print(f"ğŸ”„ ì—…ë¡œë“œ ì§„í–‰: {int(status.progress() * 100)}%")

    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! YouTube Video ID: {response.get('id')}")

    if target_kr == "ì¡°ë¹„ ì—ë¹„ì—ì´ì…˜":
        run_market_impact_pipeline()
    elif target_kr == "ì½”ì¸":
        # token.json ì‚­ì œ
        try:
            os.remove("token.json")
            print("token.json íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
        except FileNotFoundError:
            print("token.json íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•ŠìŒ.")
        except Exception as e:
            print(f"token.json ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    elif target_kr == "í…ŒìŠ¬ë¼":
        run_daily_pipeline_news_jovy()
    elif target_kr == "ê²½ì œ":
        run_daily_pipeline_news_coin()

def run_daily_pipeline_news():
    print("ğŸš€ í…ŒìŠ¬ë¼ ë‰´ìŠ¤ ìš”ì•½ ì‡¼ì¸  ìƒì„± ì‹œì‘")
    us_newsdata = fetch_newsdata_articles("tesla", country="us", language="en")
    save_articles("us", "newsdata", us_newsdata)

    collected_articles = get_news_from_html()
    summaries = summarize_articles(collected_articles, "tesla")

    if len(summaries) > 0:
        create_intro_image_news("tesla", "í…ŒìŠ¬ë¼")
        # for idx, summary in enumerate(summaries):
        #     create_body_image(summary, idx, "tesla")
        create_outro_image()

        date_str = datetime.now().strftime("%Y%m%d")

        # create_youtube_shorts_video(
        #     intro_path=OUTPUT_INTRO,
        #     body_dir=os.path.join(BASE_DIR,"results"),  # body ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë”
        #     outro_path=OUTPUT_OUTRO,
        #     bgm_path=os.path.join(BASE_DIR, "bgm", "bgm_news.mp3"),
        #     output_path=os.path.join(OUT_DIR,  f"{date_str}_tesla_news_shorts.mp4")
        # )

        create_news_shorts_video_with_bgvideo_fast(
            "tesla", summaries, BG_DIR, OUT_DIR, os.path.join(BASE_DIR, "bgm", "bgm_news.mp3"), os.path.join(OUT_DIR,  f"{date_str}_tesla_news_shorts.mp4"), duration_per_caption=3, target_kr="í…ŒìŠ¬ë¼", font_path=FONT_PATH
        )

        # â­ï¸ ë‹¤ìŒ ë‹¨ê³„: YouTube ì—…ë¡œë“œ
        upload_video_to_youtube_news(os.path.join(OUT_DIR,  f"{date_str}_tesla_news_shorts.mp4"), "í…ŒìŠ¬ë¼")
    else:
        run_daily_pipeline_news_jovy()

def run_daily_pipeline_news_jovy():
    print("ğŸš€ ì¡°ë¹„ ë‰´ìŠ¤ ìš”ì•½ ì‡¼ì¸  ìƒì„± ì‹œì‘")
    us_newsdata = fetch_newsdata_articles('Joby OR "Joby Aviation"', country="us", language="en")
    save_articles("us", "newsdata", us_newsdata)

    collected_articles = get_news_from_html()
    summaries = summarize_articles(collected_articles, "Jovy Aviation")

    if len(summaries) > 0:
        create_intro_image_news("Jovy Aviation", "ì¡°ë¹„ ì—ë¹„ì—ì´ì…˜")
        # for idx, summary in enumerate(summaries):
        #     create_body_image(summary, idx, "Jovy")
        create_outro_image()

        date_str = datetime.now().strftime("%Y%m%d")

        create_news_shorts_video_with_bgvideo_fast(
            "Jovy", summaries, BG_DIR, OUT_DIR, os.path.join(BASE_DIR, "bgm", "bgm_news.mp3"), os.path.join(OUT_DIR,  f"{date_str}_Jovy_news_shorts.mp4"), duration_per_caption=3, target_kr="ì¡°ë¹„ ì—ë¹„ì—ì´ì…˜", font_path=FONT_PATH
        )

        # â­ï¸ ë‹¤ìŒ ë‹¨ê³„: YouTube ì—…ë¡œë“œ
        upload_video_to_youtube_news(os.path.join(OUT_DIR,  f"{date_str}_Jovy_news_shorts.mp4"), "ì¡°ë¹„ ì—ë¹„ì—ì´ì…˜")
    else:
        run_market_impact_pipeline()


def run_daily_pipeline_news_business():
    print("ğŸš€ ë¹„íŠ¸ë§ˆì¸ ë‰´ìŠ¤ ìš”ì•½ ì‡¼ì¸  ìƒì„± ì‹œì‘")
    us_newsdata = fetch_newsdata_articles("bitmine", country="us", language="en", category="business")
    save_articles("us", "newsdata", us_newsdata)

    collected_articles = get_news_from_html()
    summaries = summarize_articles(collected_articles, "bitmine")

    if len(summaries) > 0:
        create_intro_image_news("bitmine", "ë¹„íŠ¸ë§ˆì¸")
        # for idx, summary in enumerate(summaries):
        #     create_body_image(summary, idx, "business")
        create_outro_image()

        date_str = datetime.now().strftime("%Y%m%d")

        create_news_shorts_video_with_bgvideo_fast(
            "bitmine", summaries, BG_DIR, OUT_DIR, os.path.join(BASE_DIR, "bgm", "bgm_news.mp3"), os.path.join(OUT_DIR,  f"{date_str}_bitmine_news_shorts.mp4"), duration_per_caption=3, target_kr="ë¹„íŠ¸ë§ˆì¸", font_path=FONT_PATH
        )

        # â­ï¸ ë‹¤ìŒ ë‹¨ê³„: YouTube ì—…ë¡œë“œ
        upload_video_to_youtube_news(os.path.join(OUT_DIR,  f"{date_str}_bitmine_news_shorts.mp4"), "ë¹„íŠ¸ë§ˆì¸")
    else:
        run_daily_pipeline_news_coin()
        

def run_daily_pipeline_news_coin():
    print("ğŸš€ ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ ìš”ì•½ ì‡¼ì¸  ìƒì„± ì‹œì‘")
    us_newsdata = fetch_newsdata_articles("bitcoin OR ethereum OR crypto", country="us", language="en", category="business")
    save_articles("us", "newsdata", us_newsdata)

    collected_articles = get_news_from_html()
    summaries = summarize_articles(collected_articles, "crypto")

    if len(summaries) > 0:
        create_intro_image_news("crypto", "ì½”ì¸")
        # for idx, summary in enumerate(summaries):
        #     create_body_image(summary, idx, "business")
        create_outro_image()

        date_str = datetime.now().strftime("%Y%m%d")

        create_news_shorts_video_with_bgvideo_fast(
            "crypto", summaries, BG_DIR, OUT_DIR, os.path.join(BASE_DIR, "bgm", "bgm_news.mp3"), os.path.join(OUT_DIR,  f"{date_str}_crypto_news_shorts.mp4"), duration_per_caption=3, target_kr="ì½”ì¸", font_path=FONT_PATH
        )

        # â­ï¸ ë‹¤ìŒ ë‹¨ê³„: YouTube ì—…ë¡œë“œ
        upload_video_to_youtube_news(os.path.join(OUT_DIR,  f"{date_str}_crypto_news_shorts.mp4"), "ì½”ì¸")
    else:
        # token.json ì‚­ì œ
        try:
            os.remove("token.json")
            print("token.json íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
        except FileNotFoundError:
            print("token.json íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•ŠìŒ.")
        except Exception as e:
            print(f"token.json ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")





# ========== ìš´ì„¸ ìƒì„± ==========
def clean_fortune_text(text):
    # 1. "ì¥ë ", "ë§ë ", "í˜¸ë‘ì´ë " ë“± ë  ì´ë¦„ ì œê±° (ë¬¸ì¥ ì‹œì‘ ìœ„ì¹˜ì—ë§Œ)
    text = re.sub(r'^[^ê°€-í£]*([ê°€-í£]{1,5}ë )[\\s:ï¼š,.~!\\-]*', r'\1 - ', text)

    # 2. ì´ëª¨ì§€ ì œê±°
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # ì´ëª¨í‹°ì½˜
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002600-\U000026FF"
        "\U00002700-\U000027BF"
        "\U0001F900-\U0001F9FF"
        "\U0001FA70-\U0001FAFF"
        "\u200d"
        "\u2640-\u2642"
        "\u23cf"
        "\u23e9-\u23f3"
        "\u25fb-\u25fe"
        "\u2614-\u2615"
        "]+",
        flags=re.UNICODE)
    return emoji_pattern.sub(r'', text).strip()


def get_daily_fortunes():
    client = OpenAI()

    prompt = (
    "ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 12ê°œ ë ë³„ ê°ê°ì˜ ìš´ì„¸ë¥¼ í•œ ë¬¸ë‹¨ì”© ì¨ì¤˜.\n"
    "ì ˆëŒ€ ë¹ ì§ì—†ì´ 12ê°œ ëª¨ë‘ ì¨ì•¼ í•˜ê³ , ë  ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:\n"
    + ", ".join(ZODIACS) + "\n\n"
    "ê° ìš´ì„¸ëŠ” ìœ íŠœë¸Œ ì‡¼ì¸ ì— ì–´ìš¸ë¦¬ëŠ” ë§íˆ¬ë¡œ, 2ë¬¸ì¥ ì •ë„ë¡œ ì§§ê³  ì¸ìƒ ê¹Šê²Œ ì¨ì¤˜.\n"
    "ë ë³„ ì´ë¦„ìœ¼ë¡œ ë¬¸ë‹¨ì„ êµ¬ë¶„í•˜ê³ , ê° ë¬¸ë‹¨ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ëˆ ì„œ ë³´ì—¬ì¤˜.\n"
    "ì˜ˆ: \n"
    "ì¥ë \nì˜¤ëŠ˜ì€ ê¸°íšŒê°€ ìˆ¨ì–´ìˆëŠ” ë‚ ì´ì—ìš”. í‰ì†Œì™€ ë‹¤ë¥¸ ì„ íƒì´ í–‰ìš´ì„ ë¶€ë¥¼ ìˆ˜ ìˆì–´ìš”.\n\n"
    "ì†Œë \në§ˆìŒì´ ì•ˆì •ë˜ê³  ì§‘ì¤‘ë ¥ì´ ë†’ì•„ì§€ëŠ” í•˜ë£¨ì˜ˆìš”. ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë¦¬ê¸° ì¢‹ì•„ìš”.\n\n"
    "ì´ í˜•ì‹ì„ ê¼­ ì§€ì¼œì„œ 12ê°œ ë ë¥¼ ì „ë¶€ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜."
    )
    res = client.chat.completions.create(model="gpt-3.5-turbo",
                                         messages=[{
                                             "role": "user",
                                             "content": prompt
                                         }],
                                         temperature=0.85)
    text = res.choices[0].message.content.strip()
    print("GPT ìš´ì„¸ ìƒì„± ê²°ê³¼:\n", text)

    fortunes = dict(zip(ZODIACS, text.split("\n\n")))
    return fortunes

def clean_emoji_text(text):
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # ì´ëª¨í‹°ì½˜
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002600-\U000026FF"
        "\U00002700-\U000027BF"
        "\U0001F900-\U0001F9FF"
        "\U0001FA70-\U0001FAFF"
        "\u200d"
        "\u2640-\u2642"
        "\u23cf"
        "\u23e9-\u23f3"
        "\u25fb-\u25fe"
        "\u2614-\u2615"
        "]+",
        flags=re.UNICODE)
    return emoji_pattern.sub(r'', text).strip()


def clean_fortune_text_star(text):
    # 1. ìë¦¬ ì´ë¦„ ì œê±° (ë¬¸ì¥ ì‹œì‘ ìœ„ì¹˜ì—ë§Œ)
    text = re.sub(r'^([^ê°€-í£]*[ê°€-í£]{1,5}ìë¦¬)[\s:ï¼š,.~!\-]*', r'\1 - ', text)
    return clean_emoji_text(text)


def get_daily_star_fortunes():
    client = OpenAI()

    prompt = (
    "ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ 12ê°œ ë³„ìë¦¬ ê°ê°ì˜ ìš´ì„¸ë¥¼ í•œ ë¬¸ë‹¨ì”© ì¨ì¤˜.\n"
    "ì ˆëŒ€ ë¹ ì§ì—†ì´ 12ê°œ ëª¨ë‘ ì¨ì•¼ í•˜ê³ , ë³„ìë¦¬ ìˆœì„œëŠ” ë‹¤ìŒê³¼ ê°™ì•„:\n"
    + ", ".join(ZODIACS_star) + "\n\n"
    "ê° ìš´ì„¸ëŠ” ìœ íŠœë¸Œ ì‡¼ì¸ ì— ì–´ìš¸ë¦¬ëŠ” ë§íˆ¬ë¡œ, 2ë¬¸ì¥ ì •ë„ë¡œ ì§§ê³  ì¸ìƒ ê¹Šê²Œ ì¨ì¤˜.\n"
    "ë³„ìë¦¬ ì´ë¦„ìœ¼ë¡œ ë¬¸ë‹¨ì„ êµ¬ë¶„í•˜ê³ , ê° ë¬¸ë‹¨ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ëˆ ì„œ ë³´ì—¬ì¤˜.\n"
    "ì˜ˆ: \n"
    "ì–‘ìë¦¬\nì˜¤ëŠ˜ì€ ê¸°íšŒê°€ ìˆ¨ì–´ìˆëŠ” ë‚ ì´ì—ìš”. í‰ì†Œì™€ ë‹¤ë¥¸ ì„ íƒì´ í–‰ìš´ì„ ë¶€ë¥¼ ìˆ˜ ìˆì–´ìš”.\n\n"
    "í™©ì†Œìë¦¬\në§ˆìŒì´ ì•ˆì •ë˜ê³  ì§‘ì¤‘ë ¥ì´ ë†’ì•„ì§€ëŠ” í•˜ë£¨ì˜ˆìš”. ì¤‘ìš”í•œ ê²°ì •ì„ ë‚´ë¦¬ê¸° ì¢‹ì•„ìš”.\n\n"
    "ì´ í˜•ì‹ì„ ê¼­ ì§€ì¼œì„œ 12ê°œ ë³„ìë¦¬ë¥¼ ì „ë¶€ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜."
    )

    res = client.chat.completions.create(model="gpt-3.5-turbo",
                                         messages=[{
                                             "role": "user",
                                             "content": prompt
                                         }],
                                         temperature=0.85)
    text = res.choices[0].message.content.strip()
    print("GPT ìš´ì„¸ ìƒì„± ê²°ê³¼:\n", text)

    fortunes = dict(zip(ZODIACS_star, text.split("\n\n")))
    return fortunes



# ì´ë¯¸ì§€ ì˜ì—­ì— ë§ì¶° ì¤„ë°”ê¿ˆ
def wrap_text(text, font, max_width):
    """
    í…ìŠ¤íŠ¸ë¥¼ ì£¼ì–´ì§„ ë„ˆë¹„(max_width)ì— ë§ì¶° ìë™ ì¤„ë°”ê¿ˆ í•´ì£¼ëŠ” í•¨ìˆ˜
    """
    lines = []
    words = text.split()

    current_line = ""
    for word in words:
        test_line = current_line + (" " if current_line else "") + word
        if font.getlength(test_line) <= max_width:
            current_line = test_line
        else:
            if current_line:
                lines.append(current_line)
            current_line = word

    if current_line:
        lines.append(current_line)

    return lines


# ì²« í˜ì´ì§€ ìƒì„±ìš©
def create_intro_image():

    now = datetime.now(ZoneInfo("Asia/Seoul"))
    date_str = f"{now.year}. {now.month}. {now.day}"  # ex: 2025. 7. 10
    line1 = f"{date_str}"
    line2 = "ë ë³„ ìš´ì„¸"

    image_path = os.path.join(BG_DIR, "first_img_ìˆì¸ .png")
    image = Image.open(image_path).convert("RGBA")
    draw = ImageDraw.Draw(image)

    font = ImageFont.truetype(FONT_PATH, FONT_SIZE * 2)
    x = image.width // 2
    y = image.height // 2

    LINE_SPACING = int(FONT_SIZE * 1.4)

    text_size = draw.textbbox((x, y), line1, font=font, anchor="mm")
    text_w = text_size[2] - text_size[0]
    text_h = text_size[3] - text_size[1]

    # ë°˜íˆ¬ëª… íšŒìƒ‰ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    box_padding = 10
    box_coords = [
        x - text_w // 2 - box_padding,
        y - (text_h * 2) // 2 - box_padding,
        x + text_w // 2 + box_padding,
        y + (text_h * 2) // 2 + box_padding,
    ]
    box_color = (75, 75, 75, 150)  # ë°˜íˆ¬ëª… íšŒìƒ‰
    overlay = Image.new("RGBA", image.size, (255, 255, 255, 0))
    overlay_draw = ImageDraw.Draw(overlay)
    overlay_draw.rectangle(box_coords, fill=box_color)
    image = Image.alpha_composite(image, overlay)

    # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
    draw = ImageDraw.Draw(image)  # ë‹¤ì‹œ draw ê°ì²´ ì¬ìƒì„±

    draw.text((x, y - LINE_SPACING // 2),
              line1,
              font=font,
              fill="black",
              anchor="mm",
              stroke_width=4,
              stroke_fill="black")
    draw.text((x, y - LINE_SPACING // 2),
              line1,
              font=font,
              fill="white",
              anchor="mm",
              stroke_width=2)

    draw.text((x, y + LINE_SPACING // 2),
              line2,
              font=font,
              fill="black",
              anchor="mm",
              stroke_width=4,
              stroke_fill="black")
    draw.text((x, y + LINE_SPACING // 2),
              line2,
              font=font,
              fill="white",
              anchor="mm",
              stroke_width=2)

    out_path = os.path.join(OUT_DIR, "0_intro.png")
    image.save(out_path)


# ========== ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ì‚½ì… ==========
def insert_fortune_text(zodiac, text):
    image_path = os.path.join(BG_DIR, f"{zodiac}ìˆì¸ .png")
    output_path = os.path.join(OUT_DIR, f"{zodiac}_ìš´ì„¸.png")

    try:
        img = Image.open(image_path).convert("RGBA")
        draw = ImageDraw.Draw(img)
        font = ImageFont.truetype(FONT_PATH, FONT_SIZE)

        x1, y1, x2, y2 = TEXT_BOX
        max_width = x2 - x1
        lines = wrap_text(text, font, max_width)

        words = text.split()
        lines, line = [], ""
        for word in words:
            test_line = f"{line} {word}".strip()
            if draw.textlength(test_line, font=font) <= max_width:
                line = test_line
            else:
                lines.append(line)
                line = word
        lines.append(line)

        LINE_SPACING = int(FONT_SIZE * 1.1)  # ê¸€ì í¬ê¸° ëŒ€ë¹„ ì¤„ ê°„ê²©

        max_lines = (y2 - y1) // LINE_SPACING
        for i, l in enumerate(lines[:max_lines]):
            y = y1 + i * LINE_SPACING
            x = (x1 + x2) // 2
            if y + LINE_SPACING > y2:
                break

            # í…ìŠ¤íŠ¸ í¬ê¸° ì¸¡ì •
            text_size = draw.textbbox((x, y), l, font=font, anchor="mm")
            text_w = text_size[2] - text_size[0]
            text_h = text_size[3] - text_size[1]

            # ë°˜íˆ¬ëª… íšŒìƒ‰ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            box_padding = 10
            box_coords = [
                x - text_w // 2 - box_padding,
                y - text_h // 2 - box_padding,
                x + text_w // 2 + box_padding,
                y + text_h // 2 + box_padding,
            ]
            box_color = (75, 75, 75, 150)  # ë°˜íˆ¬ëª… íšŒìƒ‰
            overlay = Image.new("RGBA", img.size, (255, 255, 255, 0))
            overlay_draw = ImageDraw.Draw(overlay)
            overlay_draw.rectangle(box_coords, fill=box_color)
            img = Image.alpha_composite(img, overlay)

            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            draw = ImageDraw.Draw(img)  # ë‹¤ì‹œ draw ê°ì²´ ì¬ìƒì„±

            draw.text((x, y),
                      l,
                      font=font,
                      fill="black",
                      anchor="mm",
                      stroke_width=2,
                      stroke_fill="black")
            draw.text((x, y), l, font=font, fill="white", anchor="mm")

        img.save(output_path)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
    except FileNotFoundError:
        print(f"âŒ ì´ë¯¸ì§€ ì—†ìŒ: {image_path}")



# ì²« í˜ì´ì§€ ìƒì„±ìš©
def create_star_intro_image():
    from datetime import datetime

    now = datetime.now(ZoneInfo("Asia/Seoul"))
    date_str = f"{now.year}. {now.month}. {now.day}"  # ex: 2025. 7. 10
    line1 = f"{date_str}"
    line2 = "ë³„ìë¦¬ ìš´ì„¸"

    image_path = os.path.join(BG_DIR, "first_img.png")
    image = Image.open(image_path).convert("RGBA")
    draw = ImageDraw.Draw(image)

    font = ImageFont.truetype(FONT_PATH, FONT_SIZE * 2)
    x = image.width // 2
    y = image.height // 2

    LINE_SPACING = int(FONT_SIZE * 1.4)

    text_size = draw.textbbox((x, y), line1, font=font, anchor="mm")
    text_w = text_size[2] - text_size[0]
    text_h = text_size[3] - text_size[1]

    # ë°˜íˆ¬ëª… íšŒìƒ‰ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    box_padding = 10
    box_coords = [
        x - text_w // 2 - box_padding,
        y - (text_h * 2) // 2 - box_padding,
        x + text_w // 2 + box_padding,
        y + (text_h * 2) // 2 + box_padding,
    ]
    box_color = (75, 75, 75, 150)  # ë°˜íˆ¬ëª… íšŒìƒ‰
    overlay = Image.new("RGBA", image.size, (255, 255, 255, 0))
    overlay_draw = ImageDraw.Draw(overlay)
    overlay_draw.rectangle(box_coords, fill=box_color)
    image = Image.alpha_composite(image, overlay)

    # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
    draw = ImageDraw.Draw(image)  # ë‹¤ì‹œ draw ê°ì²´ ì¬ìƒì„±

    draw.text((x, y - LINE_SPACING // 2),
              line1,
              font=font,
              fill="black",
              anchor="mm",
              stroke_width=4,
              stroke_fill="black")
    draw.text((x, y - LINE_SPACING // 2),
              line1,
              font=font,
              fill="white",
              anchor="mm",
              stroke_width=2)

    draw.text((x, y + LINE_SPACING // 2),
              line2,
              font=font,
              fill="black",
              anchor="mm",
              stroke_width=4,
              stroke_fill="black")
    draw.text((x, y + LINE_SPACING // 2),
              line2,
              font=font,
              fill="white",
              anchor="mm",
              stroke_width=2)

    out_path = os.path.join(OUT_DIR, "0_intro.png")
    image.save(out_path)


# ========== ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ì‚½ì… ==========
def insert_fortune_text_star(zodiac, text):
    image_path = os.path.join(BG_DIR, f"{zodiac}.png")
    output_path = os.path.join(OUT_DIR, f"{zodiac}ìë¦¬_ìš´ì„¸.png")

    try:
        img = Image.open(image_path).convert("RGBA")
        draw = ImageDraw.Draw(img)
        font = ImageFont.truetype(FONT_PATH, FONT_SIZE)

        x1, y1, x2, y2 = TEXT_BOX
        max_width = x2 - x1
        lines = wrap_text(text, font, max_width)

        words = text.split()
        lines, line = [], ""
        for word in words:
            test_line = f"{line} {word}".strip()
            if draw.textlength(test_line, font=font) <= max_width:
                line = test_line
            else:
                lines.append(line)
                line = word
        lines.append(line)

        LINE_SPACING = int(FONT_SIZE * 1.1)  # ê¸€ì í¬ê¸° ëŒ€ë¹„ ì¤„ ê°„ê²©

        max_lines = (y2 - y1) // LINE_SPACING
        for i, l in enumerate(lines[:max_lines]):
            y = y1 + i * LINE_SPACING
            x = (x1 + x2) // 2
            if y + LINE_SPACING > y2:
                break

            # í…ìŠ¤íŠ¸ í¬ê¸° ì¸¡ì •
            text_size = draw.textbbox((x, y), l, font=font, anchor="mm")
            text_w = text_size[2] - text_size[0]
            text_h = text_size[3] - text_size[1]

            # ë°˜íˆ¬ëª… íšŒìƒ‰ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            box_padding = 10
            box_coords = [
                x - text_w // 2 - box_padding,
                y - text_h // 2 - box_padding,
                x + text_w // 2 + box_padding,
                y + text_h // 2 + box_padding,
            ]
            box_color = (75, 75, 75, 150)  # ë°˜íˆ¬ëª… íšŒìƒ‰
            overlay = Image.new("RGBA", img.size, (255, 255, 255, 0))
            overlay_draw = ImageDraw.Draw(overlay)
            overlay_draw.rectangle(box_coords, fill=box_color)
            img = Image.alpha_composite(img, overlay)

            # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            draw = ImageDraw.Draw(img)  # ë‹¤ì‹œ draw ê°ì²´ ì¬ìƒì„±

            draw.text((x, y),
                      l,
                      font=font,
                      fill="black",
                      anchor="mm",
                      stroke_width=2,
                      stroke_fill="black")
            draw.text((x, y), l, font=font, fill="white", anchor="mm")

        img.save(output_path)
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")
    except FileNotFoundError:
        print(f"âŒ ì´ë¯¸ì§€ ì—†ìŒ: {image_path}")






# ì˜ìƒìœ¼ë¡œ ë³€í™˜
def generate_zodiac_video(image_paths,
                          out_path,
                          duration_per_image=2.5,
                          bgm_path=None):
    """
    image_paths: ìš´ì„¸ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    out_path: ì €ì¥ë  mp4 ê²½ë¡œ
    duration_per_image: ê° ì´ë¯¸ì§€ ì§€ì† ì‹œê°„ (ì´ˆ)
    bgm_path: ë°°ê²½ìŒì•… mp3 ê²½ë¡œ (ì„ íƒ)
    """
    clips = []

    for image_path in image_paths:
        clip = ImageClip(image_path, duration=duration_per_image).resize(
            height=1920).set_position("center")
        clips.append(clip)

    final_clip = concatenate_videoclips(clips, method="compose")

    if bgm_path and os.path.exists(bgm_path):
        audio = AudioFileClip(bgm_path).subclip(0, final_clip.duration)
        final_clip = final_clip.set_audio(audio)

    final_clip.write_videofile(out_path,
                               fps=30,
                               codec="libx264",
                               audio_codec="aac")


def create_daily_video_from_images():
    global timestamps
    date_str = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y%m%d")
    image_files = ["0_intro.png"] + [f"{z}_ìš´ì„¸.png" for z in ZODIACS
                                     ] + ["end_img.png"]  # ğŸ”§ ì—¬ê¸° ìˆ˜ì •ë¨

    image_paths = [
        os.path.join(OUT_DIR, f) for f in image_files
        if os.path.exists(os.path.join(OUT_DIR, f))
    ]

    video_out_path = os.path.join(OUT_DIR, f"{date_str}_zodiac_video.mp4")

    bgm_path = os.path.join(BASE_DIR, "bgm", "bgm.mp3")
    if not os.path.exists(bgm_path):
        bgm_path = None

    # â±ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    duration_per_image = 2.5
    timestamps = {}
    start_time = duration_per_image  # ì²« ë²ˆì§¸ ë ëŠ” intro(0ì´ˆ) ë‹¤ìŒì¸ 2.5ì´ˆë¶€í„° ì‹œì‘
    for zodiac in ZODIACS:
        minutes = int(start_time // 60)
        seconds = int(start_time % 60)
        timestamps[zodiac] = f"{minutes:02d}:{seconds:02d}"
        start_time += duration_per_image

    generate_zodiac_video(image_paths,
                          video_out_path,
                          duration_per_image=duration_per_image,
                          bgm_path=bgm_path)
    print(f"ğŸ¥ ì˜ìƒ ìƒì„± ì™„ë£Œ: {video_out_path}")
    return video_out_path


generated_images = []


# ============================ ìœ íŠ­ ì—…ë¡œë“œ ===========================
def upload_video_to_youtube(video_path):
    global timestamps
    creds = Credentials.from_authorized_user_file("token.json", YOUTUBE_SCOPES)
    youtube = build("youtube", "v3", credentials=creds)

    now = datetime.now(ZoneInfo("Asia/Seoul"))
    date_str = now.strftime("%Yë…„ %mì›” %dì¼")

    timestamp_description = "\n".join(
        [f"ğŸ¾ {name}ë  ìš´ì„¸ : {time}" for name, time in timestamps.items()])

    body = {
        "snippet": {
            "title": f"{date_str} ë ë³„ ìš´ì„¸ âœ¨",  # ì˜ìƒ ì œëª©
            "description":
            f"{date_str} ì˜¤ëŠ˜ì˜ ë ë³„ ìš´ì„¸ì…ë‹ˆë‹¤.\n\n{timestamp_description}\n\n#ìš´ì„¸ #ë ë³„ìš´ì„¸ #shorts",
            "tags": ["ìš´ì„¸", "ë ë³„ìš´ì„¸", "ì˜¤ëŠ˜ì˜ìš´ì„¸", "shorts"],
            "categoryId": "22"  # People & Blogs
        },
        "status": {
            "privacyStatus": "public"  # ë˜ëŠ” unlisted, private
        }
    }

    media = MediaFileUpload(video_path,
                            chunksize=-1,
                            resumable=True,
                            mimetype="video/*")

    print("ğŸ“¤ ìœ íŠœë¸Œ ì—…ë¡œë“œ ì‹œì‘...")
    request = youtube.videos().insert(part="snippet,status",
                                      body=body,
                                      media_body=media)
    response = None

    while response is None:
        status, response = request.next_chunk()
        if status:
            print(f"ğŸ”„ ì—…ë¡œë“œ ì§„í–‰: {int(status.progress() * 100)}%")

    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! YouTube Video ID: {response.get('id')}")

    # # token.json ì‚­ì œ
    # try:
    #     os.remove("token.json")
    #     print("token.json íŒŒì¼ ì‚­ì œ ì™„ë£Œ.")
    # except FileNotFoundError:
    #     print("token.json íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ì§€ ì•ŠìŒ.")
    # except Exception as e:
    #     print(f"token.json ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def run_daily_pipeline():
    print("ğŸš€ ë ë³„ ìš´ì„¸ ìƒì„± ì‹œì‘")
    create_intro_image()  # ë§¨ ì•ì¥ ì´ë¯¸ì§€ ìƒì„±
    generated_images.append(os.path.join(OUT_DIR, "0_intro.png"))

    fortunes = get_daily_fortunes()
    for zodiac in ZODIACS:
        text = fortunes.get(zodiac, "ì˜¤ëŠ˜ë„ í–‰ë³µí•œ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!")
        text = clean_fortune_text(text)
        insert_fortune_text(zodiac, text)
        image_path = os.path.join(OUT_DIR, f"{zodiac}_ìš´ì„¸.png")
        generated_images.append(image_path)

    follow_image = os.path.join(BG_DIR, "follow_prompt.png")
    if os.path.exists(follow_image):
        generated_images.append(follow_image)

    print("âœ… ì „ì²´ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")

    # ğŸ¬ ì—¬ê¸°ì„œ ì˜ìƒìœ¼ë¡œ ë³€í™˜!
    video_path = create_daily_video_from_images()

    # base64 ë¬¸ìì—´ ê°€ì ¸ì˜¤ê¸°
    token_b64 = os.getenv("TOKEN_JSON_BASE64")
    with open("token.json", "wb") as f:
        f.write(base64.b64decode(token_b64))

    # ë””ì½”ë”© í›„ token.jsonë¡œ ì €ì¥
    if token_b64:
        with open("token.json", "wb") as f:
            f.write(base64.b64decode(token_b64))
        print("token.json íŒŒì¼ ë³µì› ì™„ë£Œ.")
    else:
        print("TOKEN_JSON_BASE64 í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # â­ï¸ ë‹¤ìŒ ë‹¨ê³„: YouTube ì—…ë¡œë“œ
    upload_video_to_youtube(video_path)

    ## ë³„ìë¦¬ ìš´ì„¸ ìƒì„±
    run_daily_pipeline_star()



def create_daily_video_from_images_star():
    global timestamps
    date_str = datetime.now(ZoneInfo("Asia/Seoul")).strftime("%Y%m%d")
    image_files = ["0_intro.png"] + [f"{z}ìë¦¬_ìš´ì„¸.png" for z in ZODIACS_star
                                     ] + ["end_img.png"]  # ğŸ”§ ì—¬ê¸° ìˆ˜ì •ë¨

    image_paths = [
        os.path.join(OUT_DIR, f) for f in image_files
        if os.path.exists(os.path.join(OUT_DIR, f))
    ]

    video_out_path = os.path.join(OUT_DIR, f"{date_str}_star_video.mp4")

    bgm_path = os.path.join(BASE_DIR, "bgm", "bgm_star.mp3")
    if not os.path.exists(bgm_path):
        bgm_path = None

    # â±ï¸ íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„±
    duration_per_image = 2.5
    timestamps = {}
    start_time = duration_per_image  # ì²« ë²ˆì§¸ ë³„ìë¦¬ëŠ” intro(0ì´ˆ) ë‹¤ìŒì¸ 2.5ì´ˆë¶€í„° ì‹œì‘
    for zodiac in ZODIACS_star:
        minutes = int(start_time // 60)
        seconds = int(start_time % 60)
        timestamps[zodiac] = f"{minutes:02d}:{seconds:02d}"
        start_time += duration_per_image

    generate_zodiac_video(image_paths,
                          video_out_path,
                          duration_per_image=duration_per_image,
                          bgm_path=bgm_path)
    print(f"ğŸ¥ ì˜ìƒ ìƒì„± ì™„ë£Œ: {video_out_path}")
    return video_out_path


generated_images = []


# ============================ ìœ íŠ­ ì—…ë¡œë“œ ===========================
def upload_video_to_youtube_star(video_path):
    global timestamps
    creds = Credentials.from_authorized_user_file("token.json", YOUTUBE_SCOPES)
    youtube = build("youtube", "v3", credentials=creds)

    now = datetime.now(ZoneInfo("Asia/Seoul"))
    date_str = now.strftime("%Yë…„ %mì›” %dì¼")

    timestamp_description = "\n".join(
        [f"ğŸ¾ {name}ìë¦¬ ìš´ì„¸ : {time}" for name, time in timestamps.items()])

    body = {
        "snippet": {
            "title": f"{date_str} ë³„ìë¦¬ ìš´ì„¸ âœ¨",  # ì˜ìƒ ì œëª©
            "description":
            f"{date_str} ì˜¤ëŠ˜ì˜ ë³„ìë¦¬ ìš´ì„¸ì…ë‹ˆë‹¤.\n\n{timestamp_description}\n\n#ìš´ì„¸ #ë³„ìë¦¬ìš´ì„¸ #shorts",
            "tags": ["ìš´ì„¸", "ë³„ìë¦¬ìš´ì„¸", "ì˜¤ëŠ˜ì˜ìš´ì„¸", "shorts"],
            "categoryId": "22"  # People & Blogs
        },
        "status": {
            "privacyStatus": "public"  # ë˜ëŠ” unlisted, private
        }
    }

    media = MediaFileUpload(video_path,
                            chunksize=-1,
                            resumable=True,
                            mimetype="video/*")

    print("ğŸ“¤ ìœ íŠœë¸Œ ì—…ë¡œë“œ ì‹œì‘...")
    request = youtube.videos().insert(part="snippet,status",
                                      body=body,
                                      media_body=media)
    response = None

    while response is None:
        status, response = request.next_chunk()
        if status:
            print(f"ğŸ”„ ì—…ë¡œë“œ ì§„í–‰: {int(status.progress() * 100)}%")

    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! YouTube Video ID: {response.get('id')}")

    run_daily_pipeline_news()


def run_daily_pipeline_star():
    print("ğŸš€ ë³„ìë¦¬ ìš´ì„¸ ìƒì„± ì‹œì‘")
    create_star_intro_image()  # ë§¨ ì•ì¥ ì´ë¯¸ì§€ ìƒì„±
    generated_images.append(os.path.join(OUT_DIR, "0_intro.png"))

    fortunes = get_daily_star_fortunes()
    for zodiac in ZODIACS_star:
        text = fortunes.get(zodiac, "ì˜¤ëŠ˜ë„ í–‰ë³µí•œ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!")
        text = clean_fortune_text_star(text)
        insert_fortune_text_star(zodiac, text)
        image_path = os.path.join(OUT_DIR, f"{zodiac}ìë¦¬_ìš´ì„¸.png")
        generated_images.append(image_path)

    follow_image = os.path.join(BG_DIR, "follow_prompt.png")
    if os.path.exists(follow_image):
        generated_images.append(follow_image)

    print("âœ… ì „ì²´ ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")

    # ğŸ¬ ì—¬ê¸°ì„œ ì˜ìƒìœ¼ë¡œ ë³€í™˜!
    video_path = create_daily_video_from_images_star()

    # â­ï¸ ë‹¤ìŒ ë‹¨ê³„: YouTube ì—…ë¡œë“œ
    upload_video_to_youtube_star(video_path)



# ========== new ìì‚° ê´€ë ¨ ì‹ ê·œ ë‰´ìŠ¤ í•¨ìˆ˜ ======

# -----------------------
# ë³´ì¡° ìœ í‹¸: ë‚ ì§œ íŒŒì‹± ì•ˆì „ í•¨ìˆ˜
# -----------------------
def parse_date_flexible(s: str):
    """ì—¬ëŸ¬ í¬ë§·ì„ ì‹œë„í•´ì„œ datetime ë°˜í™˜ (UTC+9 ê¸°ì¤€ìœ¼ë¡œ ë°˜í™˜). ì‹¤íŒ¨ ì‹œ None."""
    if s is None:
        return None
    # ì´ë¯¸ ISO í˜•íƒœì¼ ê°€ëŠ¥ì„±
    try:
        # ì¼ë¶€ APIëŠ” '2025-08-01T12:34:56Z' ë˜ëŠ” '2025-08-01 12:34:56' ë“±ìœ¼ë¡œ ì œê³µ.
        # datetime.fromisoformatì€ Zë¥¼ ëª» ë°›ìœ¼ë¯€ë¡œ replace ì²˜ë¦¬
        txt = s.strip()
        txt = txt.replace("Z", "+00:00")
        dt = None
        try:
            dt = datetime.fromisoformat(txt)
        except Exception:
            pass
        if dt is None:
            # fallback common formats
            for fmt in ("%Y-%m-%dT%H:%M:%S%z", "%Y-%m-%d %H:%M:%S%z", "%Y-%m-%d %H:%M:%S", "%a, %d %b %Y %H:%M:%S %Z"):
                try:
                    dt = datetime.strptime(txt, fmt)
                    break
                except Exception:
                    continue
        if dt is None:
            # try numeric-only fallback
            nums = re.findall(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}", txt)
            if nums:
                dt = datetime.fromisoformat(nums[0])
        if dt is None:
            return None
        # If no tzinfo, assume UTC then convert to Asia/Seoul
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        # convert to Asia/Seoul (KST)
        try:
            return dt.astimezone(ZoneInfo("Asia/Seoul"))
        except Exception:
            # zoneinfo ì—†ìŒ â†’ ì•ˆì „ fallback
            kst = dt + timedelta(hours=9)
            return kst.replace(tzinfo=None)   # tzinfo ê°•ì œ ì„¤ì •í•˜ì§€ ì•ŠìŒ
    except Exception:
        return None
    
# -----------------------
# ë³´ì¡°: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³  ì‹œê°„ ë²”ìœ„ë¡œ í•„í„°
# -----------------------
def collect_recent_articles(from_dt: datetime, to_dt: datetime) -> List[Dict[str, Any]]:
    """
    ê¸°ì¡´ì˜ fetch_newsdata_articles / fetch_rss_articles ë“± ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë“¤ì„ ì¬ì‚¬ìš©í•´ì„œ
    from_dt <= published <= to_dt ë²”ìœ„ì˜ ê¸°ì‚¬ë“¤ì„ ìˆ˜ì§‘í•˜ì—¬ ë°˜í™˜.
    ê²°ê³¼ í•­ëª© dict: {title, content, url, source, published}
    """
    collected = []

    # 1) NewsData (ì˜ˆ: country combinations)
    try:
        # ê°€ëŠ¥í•œ ì¡°í•©: kr(ko), us(en), global/en ë“± â€” ê¸°ì¡´ í™˜ê²½ì— ë§ê²Œ í˜¸ì¶œ
        try:
            us = fetch_newsdata_articles(q=None, country="us", language="en", category="business") or []
            print("NewsData US articles fetched:", len(us))
            print("usê°€ ìˆìŒì—ë„ ì•„ë˜ì—ì„œ collected appendê°€ ì•ˆë˜ê³  ìˆìŒ, us- :")
            print(us)
            for a in us:
                pub = a.get("pubDate") or a.get("published_at") or a.get("date")
                print("pub == ", pub)
                pub_dt = parse_date_flexible(pub)
                print("pub_dt == ", pub_dt)
                # if pub_dt and from_dt <= pub_dt <= to_dt:
                collected.append({
                    "title": a.get("title"),
                    "content": a.get("content") or a.get("description") or a.get("summary") or "",
                    "url": a.get("link") or a.get("url") or a.get("source_url"),
                    "source": a.get("source_id") or a.get("source", {}).get("name") if isinstance(a.get("source"), dict) else a.get("source"),
                    "published": pub_dt.isoformat()
                })
        except Exception as e:
            print("NewsData US fetch error:", e)

        try:
            kr = fetch_newsdata_articles(q=None, country="kr", language="ko", category="business") or []
            print("NewsData KR articles fetched:", len(kr))
            print("krê°€ ìˆìŒì—ë„ ì•„ë˜ì—ì„œ collected appendê°€ ì•ˆë˜ê³  ìˆìŒ, kr- :")
            print(kr)
            for a in kr:
                pub = a.get("pubDate") or a.get("published_at") or a.get("date")
                print("pub == ", pub)
                pub_dt = parse_date_flexible(pub)
                print("pub_dt == ", pub_dt)
                # if pub_dt and from_dt <= pub_dt <= to_dt:
                collected.append({
                    "title": a.get("title"),
                    "content": a.get("content") or a.get("description") or a.get("summary") or "",
                    "url": a.get("link") or a.get("url"),
                    "source": a.get("source_id") or a.get("source"),
                    "published": pub_dt.isoformat()
                })
        except Exception as e:
            print("NewsData KR fetch error:", e)
    except Exception as e:
        print("NewsData fetch general error:", e)

    # 2) RSS í”¼ë“œ (ê¸°ì¡´ fetch_rss_articlesë¥¼ regionë³„ë¡œ)
    try:
        for region in ("kr", "global"):
            try:
                rss_list = fetch_rss_articles(region) or []
                print("RSSë„ ê³„ì† ì—†ë‹¤ê³  í•¨, rss list ====== ")
                print(rss_list)
                for a in rss_list:
                    pub = a.get("published")
                    print("pub == ", pub)
                    pub_dt = parse_date_flexible(pub)
                    print("pub_dt == ", pub_dt)
                    # if pub_dt and from_dt <= pub_dt <= to_dt:
                    collected.append({
                        "title": a.get("title"),
                        "content": a.get("summary") or "",
                        "url": a.get("link"),
                        "source": region,
                        "published": pub_dt.isoformat()
                    })
            except Exception as e:
                print("RSS fetch error for region", region, e)
        print("all articles fetched:", len(collected))
    except Exception as e:
        print("RSS fetch general error", e)

    # dedupe by URL or title
    seen = set()
    deduped = []
    for art in collected:
        key = (art.get("url") or "")[:200] + "|" + (art.get("title") or "")[:200]
        if key in seen:
            continue
        seen.add(key)
        deduped.append(art)
    print(f"[collect_recent_articles] collected {len(deduped)} articles in time range.")
    return deduped


# -----------------------
# GPTì—ê²Œ ë¶„ì„ ìš”ì²­í•˜ëŠ” í•¨ìˆ˜
# -----------------------
def ask_gpt_market_impact(articles: List[Dict[str,Any]], from_dt: datetime, to_dt: datetime) -> Dict[str, List[Dict[str,Any]]]:
    """
    articles: list of {title, content, url, source, published}
    ë°˜í™˜: dict mapping asset category -> list of items:
      { "stocks": [ {title, summary, impact_score, rationale, url, published}, ... ], "crypto": [...], ... }
    """

    # asset categories ìˆœì„œ (ì›í•˜ë©´ ì¶”ê°€)
    asset_categories = ["Stocks", "Gold", "Crypto", "RealEstate", "Forex", "Bonds", "Commodities", "Other"]

    # Build prompt
    # ì•½ì‹: ì „ë‹¬ë°›ì€ ê¸°ì‚¬ ëª©ë¡(ì œëª©+ìš”ì•½+url+source+published)ì„ ë„£ê³  -> ê° assetì— ëŒ€í•´ì„œ
    # ì˜í–¥ì„ ì¤„ë§Œí•œ ê¸°ì‚¬ë“¤ì„ ë½‘ê³  ì˜í–¥ì •ë„(0-100), ì§§ì€ ìš”ì•½, ê°„ë‹¨ ê·¼ê±°ë¥¼ JSON í¬ë§·ìœ¼ë¡œ ë‹¬ë¼
    article_texts = []
    for i, a in enumerate(articles, start=1):
        article_texts.append(f"{i}. TITLE: {a.get('title')}\nURL: {a.get('url')}\nPUBLISHED: {a.get('published')}\nSOURCE: {a.get('source')}\nCONTENT: {a.get('content')[:2000]}\n---")
    big_block = "\n".join(article_texts)

    prompt = (
        "Please answer everything in Korean.\n"
        "You are an expert market analyst. Based on the following collection of news articles (titles, short content, urls and published times),\n"
        "identify which articles within the given 48-hour window are likely to have meaningful impact on specific asset markets (Stocks, Gold, Crypto, RealEstate, Forex, Bonds, Commodities, Other).\n"
        "For each asset category, please produce a JSON object mapping the category name to an array of objects. Each object must include:\n"
        "  - title: a short title (from the article)\n"
        "  - summary: a concise summary focusing on how it affects that asset (max ~250 chars)\n"
        "  - impact_score: integer 0-100 estimating the magnitude of impact on that asset\n"
        "  - rationale: 1-2 sentence justification why it affects that asset\n"
        "  - url: original url\n"
        "  - published: published datetime in ISO format\n\n"
        "Return only valid JSON and nothing else. Use the following category order: " + ", ".join(asset_categories) + ".\n\n"
        "Articles (only those within last 48 hours):\n" + big_block + "\n\n"
        "Important: Keep the JSON compact but valid. If a category has no relevant articles, return an empty array for it.\n"
    )

    # Call GPT (using user's existing style)
    try:
        response = openai.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role":"system", "content":"You are a market research analyst."},
                {"role":"user", "content": prompt}
            ],
            temperature=0
        )
        raw = response.choices[0].message.content.strip()
        print(">>>>>>>> GPT raw response:\n", raw[:1000])
    except Exception as e:
        print("GPT call error:", e)
        return {}

    # try to extract JSON substring
    json_text = raw
    # if raw contains code fences or text around json, extract {...}
    m = re.search(r'(\{.*\})', raw, flags=re.S)
    if m:
        json_text = m.group(1)

    try:
        parsed = json.loads(json_text)
        # normalize keys -> ensure each asset_category present
        normalized = {}
        for cat in asset_categories:
            if cat in parsed:
                items = parsed[cat]
            elif cat.lower() in parsed:
                items = parsed[cat.lower()]
            else:
                items = []
            normalized[cat] = items
        print(">>>>>>>> GPT parsed and normalized JSON:\n",normalized)
        return normalized
    except Exception as e:
        print("JSON parse failed:", e)
        print("raw GPT output:\n", raw[:2000])
        return {}
    
# -----------------------
# í˜ì´ì§€(í”„ë ˆì„) ìƒì„± í•¨ìˆ˜: ìì‚°ë³„ title + numbered items
# -----------------------
def build_pages_for_assets(assets_dict: Dict[str, List[Dict[str,Any]]], max_chars_per_frame=120) -> List[str]:
    """
    Return list of 'page texts' in the order:
      [ asset_title_1, asset1_item1_short, asset1_item2_short, ..., asset_title_2, asset2_item1_short, ... ]
    Each entry is a single string that will be converted to an image/frame.
    Splits long summaries into multiple frames by max_chars_per_frame.
    """
    pages = []
    for asset, items in assets_dict.items():
        # asset title page
        pages.append(asset)  # will be rendered as a title page
        # numbered items
        for i, it in enumerate(items, start=1):
            # build "1) summary (score:xx)"
            title = it.get("title") or ""
            summary = it.get("summary") or ""
            score = it.get("impact_score") or ""
            combined = f"{i}) {summary} (Impact: {score})"
            # split combined if too long
            if len(combined) <= max_chars_per_frame:
                pages.append(combined)
            else:
                # split into chunks at word boundaries
                words = combined.split()
                chunk = ""
                for w in words:
                    if len(chunk) + 1 + len(w) <= max_chars_per_frame:
                        chunk = (chunk + " " + w).strip()
                    else:
                        pages.append(chunk)
                        chunk = w
                if chunk:
                    pages.append(chunk)
    return pages

# -----------------------
# ë¹„ë””ì˜¤ ë¹Œë“œ + ì—…ë¡œë“œ : pages -> short video (ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì§)
# -----------------------
def build_and_save_shorts_video_from_pages(pages: List[str],
                                           bg_dir: str,
                                           out_dir: str,
                                           bgm_path: str,
                                           output_path: str,
                                           font_path: str = None):
    """
    ì´ í•¨ìˆ˜ëŠ” pages ë¦¬ìŠ¤íŠ¸(ê° í˜ì´ì§€ëŠ” ë¬¸ì)ë¥¼ ë°›ì•„ì„œ
    ì´ì „ create_news_shorts_video_with_bgvideo_fastì˜ 'ë°°ê²½ ì´ì–´ë¶™ì´ê¸°' ë¡œì§ì„ ì‚¬ìš©í•´
    ì˜ìƒìœ¼ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. (ì¸íŠ¸ë¡œ/ì•„ì›ƒíŠ¸ë¡œ ì´ë¯¸ì§€ëŠ” ê¸°ì¡´ ê²ƒ ì‚¬ìš©)
    """
    # prepare intro/outro
    intro_img_path = OUTPUT_INTRO if os.path.exists(OUTPUT_INTRO) else os.path.join(bg_dir, "intro_bg_business.png")
    outro_img_path = OUTRO_BG if os.path.exists(OUTRO_BG) else os.path.join(bg_dir, "outro_bg.png")

    clips = []
    intro_clip = ImageClip(intro_img_path).set_duration(3).resize((1080,1920))
    clips.append(intro_clip)

    # prepare bg clips list â€” reuse earlier logic: create a list longer than needed
    video_candidates = [f for f in os.listdir(bg_dir) if f.endswith(".mp4")]
    if not video_candidates:
        raise FileNotFoundError("Background mp4 files not found in bg_dir")

    # create a list of background filenames (randomized)
    # make length > pages count to reduce immediate repeat
    bg_file_list = []
    for i in range(max(len(pages), len(video_candidates)) + 3):
        bg_file_list.append(random.choice(video_candidates))
    random.shuffle(bg_file_list)

    # load VideoFileClip objects
    bg_video_clips = [VideoFileClip(os.path.join(bg_dir, f)).resize((1080,1920)) for f in bg_file_list]
    bg_index = 0
    bg_pos = 0.0

    # total duration allocation similar to earlier: compute per-page duration so final <= 60s
    intro_duration = 3
    outro_duration = 2
    total_max = 60
    available = total_max - intro_duration - outro_duration
    per_page = available / max(1, len(pages))
    per_page = max(1.0, min(4.0, per_page))  # keep between 1 and 4 sec (pages can be many)

    for p in pages:
        caption_array = create_caption_image_array(p, size=(1080,1920), font_path=font_path)
        caption_clip = ImageClip(caption_array, transparent=True).set_duration(per_page)
        remaining = per_page
        subclips = []
        while remaining > 0:
            cur_clip = bg_video_clips[bg_index]
            avail = cur_clip.duration - bg_pos
            use = min(avail, remaining)
            if use <= 0:
                bg_index = (bg_index + 1) % len(bg_video_clips)
                bg_pos = 0.0
                continue
            sub = cur_clip.subclip(bg_pos, bg_pos + use)
            subclips.append(sub)
            bg_pos += use
            remaining -= use
            if bg_pos >= cur_clip.duration - 1e-6:
                bg_index = (bg_index + 1) % len(bg_video_clips)
                bg_pos = 0.0
        bg_clip = concatenate_videoclips(subclips)
        comp = CompositeVideoClip([bg_clip, caption_clip])
        clips.append(comp)

    # outro
    outro_clip = ImageClip(outro_img_path).set_duration(2).resize((1080,1920))
    clips.append(outro_clip)

    final = concatenate_videoclips(clips, method="compose")
    if bgm_path and os.path.exists(bgm_path):
        bgm = AudioFileClip(bgm_path).volumex(0.5)
        final = final.set_audio(bgm.set_duration(final.duration))

    # ensure out_dir exists
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    final.write_videofile(output_path, fps=30, codec='libx264', audio_codec='aac')
    return output_path

# -----------------------
# ìµœì¢… í†µí•© í•¨ìˆ˜: ìš”êµ¬ì‚¬í•­(1)-(4)ì„ ìˆ˜í–‰
# -----------------------
def run_market_impact_pipeline():
    print("ğŸš€ ì‹œì¥ ì˜í–¥ ë‰´ìŠ¤ ì‡¼ì¸  ìƒì„± ì‹œì‘")
    """
    1) now ê¸°ì¤€ìœ¼ë¡œ 48ì‹œê°„ ì „ë¶€í„° nowê¹Œì§€ ê¸°ì‚¬ ìˆ˜ì§‘
    2) GPTì—ê²Œ ê° ìì‚°ì‹œì¥ë³„ ì˜í–¥ ë¶„ì„ ìš”ì²­ (JSON)
    3) ê²°ê³¼ë¥¼ í˜ì´ì§€ë¡œ ë³€í™˜(ìì‚°ë³„ íƒ€ì´í‹€ í•œ ì¥ + ë²ˆí˜¸ ë§¤ê¸´ ìš”ì•½ ì¥ë“¤)
    4) ê¸°ì¡´ ìŠ¤íƒ€ì¼ì˜ ì‡¼ì¸  ìƒì„± & ìœ íŠœë¸Œ ì—…ë¡œë“œ
    """
    now = datetime.now(tz=ZoneInfo("Asia/Seoul"))
    from_dt = now - timedelta(hours=48)
    print(f"[market pipeline] Time window: {from_dt.isoformat()} ~ {now.isoformat()}")

    # 1) collect
    articles = collect_recent_articles(from_dt, now)

    if not articles:
        print("[market pipeline] ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì—†ìŒ â€” ì¢…ë£Œ")
        run_daily_pipeline_news_coin()
        return

    # 2) ask GPT
    assets_analysis = ask_gpt_market_impact(articles, from_dt, now)
    if not assets_analysis:
        print("[market pipeline] GPTì—ì„œ ìœ íš¨í•œ ë¶„ì„ì„ ë°›ì§€ ëª»í•¨ â€” ì¢…ë£Œ")
        run_daily_pipeline_news_coin()
        return
    print("[market pipeline] GPT ë¶„ì„ ê²°ê³¼:", assets_analysis)

    # 3) build pages
    pages = build_pages_for_assets(assets_analysis, max_chars_per_frame=120)
    if not pages:
        print("[market pipeline] ìƒì„±ëœ í˜ì´ì§€ ì—†ìŒ â€” ì¢…ë£Œ")
        run_daily_pipeline_news_coin()
        return
    print("[market pipeline] ìƒì„±ëœ í˜ì´ì§€:", pages)

    # create output filename
    date_str = now.strftime("%Y%m%d_%H%M")
    out_filename = os.path.join(OUT_DIR, f"{date_str}_market_impact_shorts.mp4")
    bgm_file = os.path.join(BASE_DIR, "bgm", "bgm_news2.mp3") if 'BASE_DIR' in globals() else None

    # 4) build video
    print("[market pipeline] í˜ì´ì§€ ìˆ˜:", len(pages))
    video_path = build_and_save_shorts_video_from_pages(pages, BG_DIR, OUT_DIR, bgm_file, out_filename, font_path=FONT_PATH)

    # 5) upload (reuse existing uploader, pass target string "ì‹œì¥ìš”ì•½")
    upload_video_to_youtube_news(video_path, "íˆ¬ìê´€ë ¨ë‰´ìŠ¤")

    print("[market pipeline] ì™„ë£Œ: ", video_path)

    run_daily_pipeline_news_coin()



# ========== ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • ==========
# seoul_tz = timezone('Asia/Seoul')

# scheduler = BackgroundScheduler(timezone=seoul_tz)
# scheduler.add_job(run_daily_pipeline, 'cron', hour=8, minute=0)
# scheduler.start()

# ========== Flask ì›¹ì„œë²„ for UptimeRobot ==========
# app = Flask(__name__)


# @app.route("/")
# def home():
#     return "âœ… Zodiac bot is alive!"


if __name__ == "__main__":

    run_daily_pipeline()  # ìˆ˜ë™ ì‹¤í–‰ë„ ê°€ëŠ¥

    # scheduler.start()
    # app.run(host="0.0.0.0", port=8080)
